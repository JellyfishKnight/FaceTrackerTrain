import torch
import torch.nn as nn
import argparse
import onnx
import onnxruntime
from model import FacialActionModelWithAuxLoss  # ç¡®ä¿å¯¼å…¥ä½ çš„æ¨¡å‹ç±»

def load_model(pth_path, num_classes=43, device="cpu"):
    """åŠ è½½ PyTorch æ¨¡å‹"""
    model = FacialActionModelWithAuxLoss(num_classes=num_classes, backbone='shufflenet', pretrained=True)
    model.load_state_dict(torch.load(pth_path, map_location=device))
    model.to(device)
    model.eval()  # è®¾ä¸ºæ¨ç†æ¨¡å¼
    print(f"âœ… æ¨¡å‹ {pth_path} åŠ è½½å®Œæˆ")
    return model

def export_to_onnx(model, onnx_path, input_size=(1, 3, 224, 224), device="cpu"):
    """å¯¼å‡º PyTorch æ¨¡å‹ä¸º ONNX"""
    dummy_input = torch.randn(*input_size).to(device)  # åˆ›å»ºéšæœºè¾“å…¥
    torch.onnx.export(
        model, 
        dummy_input, 
        onnx_path, 
        export_params=True,  # ä¿å­˜å‚æ•°
        opset_version=11,  # ONNX ç‰ˆæœ¬
        do_constant_folding=True,  # å¸¸é‡æŠ˜å ä¼˜åŒ–
        input_names=["input"], 
        output_names=["output"],
        dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}}  # å…è®¸åŠ¨æ€ batch
    )
    print(f"ğŸ‰ æˆåŠŸå¯¼å‡º ONNX: {onnx_path}")

def verify_onnx(onnx_path):
    """éªŒè¯ ONNX æ¨¡å‹"""
    model = onnx.load(onnx_path)
    onnx.checker.check_model(model)
    print(f"âœ… ONNX æ¨¡å‹ {onnx_path} é€šè¿‡éªŒè¯")

def run_onnx_inference(onnx_path, input_size=(1, 3, 224, 224)):
    """ä½¿ç”¨ ONNX Runtime è¿è¡Œæ¨ç†æµ‹è¯•"""
    ort_session = onnxruntime.InferenceSession(onnx_path)
    dummy_input = torch.randn(*input_size).numpy()
    
    outputs = ort_session.run(None, {"input": dummy_input})
    print(f"ğŸ” ONNX æ¨ç†æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {outputs[0].shape}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--pth", type=str, required=True, help="è¾“å…¥çš„ PyTorch .pth æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--onnx", type=str, default="model.onnx", help="è¾“å‡º ONNX æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", type=str, choices=["cpu", "cuda"], default="cpu", help="è¿è¡Œè®¾å¤‡")
    args = parser.parse_args()

    # åŠ è½½ PyTorch æ¨¡å‹
    model = load_model(args.pth, device=args.device)

    # å¯¼å‡º ONNX
    export_to_onnx(model, args.onnx, device=args.device)

    # éªŒè¯ ONNX
    verify_onnx(args.onnx)

    # è¿è¡Œ ONNX æ¨ç†æµ‹è¯•
    run_onnx_inference(args.onnx)
